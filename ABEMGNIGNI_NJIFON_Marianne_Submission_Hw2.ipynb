{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plot_lib import plot_data, plot_model, set_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from torchsummary import summary\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the folder \"pictures\" where I will save the reconstructed images\n",
    "path = './pictures'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert vector to image\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.view(x.size(0), 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Displaying routine\n",
    "\n",
    "def display_images(in_, out, n=1):\n",
    "    for N in range(n):\n",
    "        if in_ is not None:\n",
    "            in_pic = to_img(in_.cpu().data)\n",
    "            plt.figure(figsize=(18, 6))\n",
    "            for i in range(4):\n",
    "                plt.subplot(1,4,i+1)\n",
    "                plt.imshow(in_pic[i+4*N])\n",
    "                plt.axis('off')\n",
    "        out_pic = to_img(out.cpu().data)\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        for i in range(4):\n",
    "            plt.subplot(1,4,i+1)\n",
    "            plt.imshow(out_pic[i+4*N])\n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define data loading step\n",
    "\n",
    "batch_size =  128 #256\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = FashionMNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model architecture and reconstruction loss\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential( nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=3,  padding=1),\n",
    "           nn.LeakyReLU(), nn.AvgPool2d( kernel_size=2,stride=2,  padding=0 ), \n",
    "        nn.Conv2d(16, 8, kernel_size=3, stride=2,  padding=1) , nn.LeakyReLU(),\n",
    "            nn.AvgPool2d( kernel_size=2, stride=1,  padding=0 )\n",
    "        )\n",
    "        self.decoder = nn.Sequential( nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2,  padding=0), \n",
    "            nn.LeakyReLU(), nn.ConvTranspose2d(16, 8, kernel_size=5, stride=3,  padding=1), \n",
    "            nn.LeakyReLU(), nn.ConvTranspose2d( 8, 1, kernel_size=2, stride=2,  padding=1), nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        #self.fc =  nn.Linear(8*2*2, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 10, 10]             160\n",
      "         LeakyReLU-2           [-1, 16, 10, 10]               0\n",
      "         AvgPool2d-3             [-1, 16, 5, 5]               0\n",
      "            Conv2d-4              [-1, 8, 3, 3]           1,160\n",
      "         LeakyReLU-5              [-1, 8, 3, 3]               0\n",
      "         AvgPool2d-6              [-1, 8, 2, 2]               0\n",
      "   ConvTranspose2d-7             [-1, 16, 5, 5]           1,168\n",
      "         LeakyReLU-8             [-1, 16, 5, 5]               0\n",
      "   ConvTranspose2d-9            [-1, 8, 15, 15]           3,208\n",
      "        LeakyReLU-10            [-1, 8, 15, 15]               0\n",
      "  ConvTranspose2d-11            [-1, 1, 28, 28]              33\n",
      "             Tanh-12            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 5,729\n",
      "Trainable params: 5,729\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Print the parameters size of the model\n",
    "summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(3, 3), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): ConvTranspose2d(16, 8, kernel_size=(5, 5), stride=(3, 3), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): ConvTranspose2d(8, 1, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure the optimiser\n",
    "\n",
    "learning_rate = 1e-3\n",
    "L2_regularization = 1e-5\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate, weight_decay = L2_regularization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/20], loss : 0.15426026284694672\n",
      "epoch[2/20], loss : 0.10638400167226791\n",
      "epoch[3/20], loss : 0.1068296954035759\n",
      "epoch[4/20], loss : 0.1015646681189537\n",
      "epoch[5/20], loss : 0.09650999307632446\n",
      "epoch[6/20], loss : 0.08459876477718353\n",
      "epoch[7/20], loss : 0.08838369697332382\n",
      "epoch[8/20], loss : 0.09778234362602234\n",
      "epoch[9/20], loss : 0.08739129453897476\n",
      "epoch[10/20], loss : 0.08597636967897415\n",
      "epoch[11/20], loss : 0.0928310677409172\n",
      "epoch[12/20], loss : 0.07788118720054626\n",
      "epoch[13/20], loss : 0.08396041393280029\n",
      "epoch[14/20], loss : 0.09564739465713501\n",
      "epoch[15/20], loss : 0.07875486463308334\n",
      "epoch[16/20], loss : 0.08217321336269379\n",
      "epoch[17/20], loss : 0.07887201011180878\n",
      "epoch[18/20], loss : 0.081071637570858\n",
      "epoch[19/20], loss : 0.0847182422876358\n",
      "epoch[20/20], loss : 0.0804489329457283\n"
     ]
    }
   ],
   "source": [
    "# Train standard or denoising autoencoder (AE)\n",
    "\n",
    "num_epochs = 20\n",
    "# do = nn.Dropout()  # comment out for standard AE\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, label = data\n",
    "        img.requires_grad_()\n",
    "        #img = img.view(img.size(0), -1)\n",
    "        img = img.view(-1,1,28,28)\n",
    "#         img_bad = do(img).to(device)  # comment out for standard AE\n",
    "        # ===================forward=====================\n",
    "        output = model(img)  # feed <img> (for std AE) or <img_bad> (for denoising AE)\n",
    "        #output = model(label)\n",
    "        #output = output.view(-1,1,28,28)\n",
    "        loss = criterion(output, img.data)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch[{}/{}], loss : {}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "    #Saving the images at each epoch\n",
    "    save_image(output, \"./pictures/{}.png\".format(epoch+1))\n",
    "    \n",
    "    #displaying the images\n",
    "    display_images(None, output)  # pass (None, output) for std AE, (img_bad, output) for denoising AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
